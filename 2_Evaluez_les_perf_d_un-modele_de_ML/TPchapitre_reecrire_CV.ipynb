{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graphic-netscape",
   "metadata": {},
   "source": [
    "# TP Chapitre - Entraînez-vous : implémentez une validation croisée\n",
    "\n",
    "This the notebook where I constructed the method, step by step, so there are probably repeated parts of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adopted-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"~/Documents/openclassroom/Fomation_ingenieur_ML/data/\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## sklearn module : \n",
    "from sklearn import model_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ranking-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+'winequality-red+.csv', sep=\";\")\n",
    "\n",
    "## DESIGN AND SAMPLING : \n",
    "X = df.drop(\"quality\", axis = 1, inplace = False)\n",
    "threshold = np.floor(df.quality.mean())\n",
    "y = np.where(df.quality < threshold, 0,1) \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "#### comment je créée mes conteneurs pour extraire les données train/test ?\n",
    "## PAR UN DICT GLOBAL ?\n",
    "# original_data_split={\"X_train\":X_train,\"X_test\":X_test,\n",
    "#                      \"y_train\":y_train, \"y_test\":y_test}\n",
    "# original_data_split[\"X_train\"]\n",
    "\n",
    "# class DataSplit:\n",
    "#    def __init__(self):\n",
    "#     X_train = self.X_train\n",
    "#     y_train = self.y_train\n",
    "#     X_test = self.X_test\n",
    "#     y_test = self.y_test\n",
    "    \n",
    "    \n",
    "## EXTRACT X_train, X_test FROM X AND train_index :    \n",
    "def extract_Xsplitted_data(X, train_index):    \n",
    "#     train_index = X_train.index\n",
    "    test_index = [(index not in train_index) for index in X.index]\n",
    "    X_train = X.loc[train_index]\n",
    "    X_test = X.loc[test_index]\n",
    "    return(X_train, X_test)\n",
    "\n",
    "## STANDARDIZATION : \n",
    "def normalize_from_split(X, train_index):\n",
    "    X_train, X_test = extract_Xsplitted_data(X, train_index)\n",
    "    \n",
    "    my_normalizer = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_std = pd.DataFrame(my_normalizer.transform(X_train), index = X_train.index, columns = X_train.columns)\n",
    "    X_test_std = pd.DataFrame(my_normalizer.transform(X_test), index = X_test.index, columns = X_test.columns)\n",
    "\n",
    "    return(X_train_std, X_test_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-grain",
   "metadata": {},
   "source": [
    "###### LIST OF ARGUMENTS AT THE END \n",
    "clf_meth = neighbors.KNeighborsClassifier()\n",
    "\n",
    "param_grid = {\"n_neighbors\" : np.arange(2,15,1)}\n",
    "\n",
    "cv = 5\n",
    "\n",
    "###### WHAT IT DOES : \n",
    "1. Create \"cv\"(=5 for ex) folds \n",
    "    \n",
    "    1.1 Separate the observations in 5 parts that respect the global distrib (folds)\n",
    "\n",
    "    1.2 Loop for fold_f in 1:5, \n",
    "            fit the clf_meth model on the Xcv_train = X_train[all fold - fold_f],\n",
    "    \n",
    "            loop for params in param_grid.keys() : \n",
    "            \n",
    "                fit clf_meth(Xcv_train, ycv_train\n",
    "            \n",
    "            save results for prediction on Xcv_test = X_train[fold_f] \n",
    "\n",
    "2. define class methods : \n",
    "    \n",
    "    2.1 cv_results_ : save mean results of models in the dict param_grid.key()\n",
    "    \n",
    "    2.2 best_params_ : the better parameter set in param_grid, in term of accuracy\n",
    "    ... many other methods, see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-programmer",
   "metadata": {},
   "source": [
    "In the chapter \"Mettez en place un cadre de validation croisée\", they explain all the approach of the Cross Validation, from the separation in kflod, with the \"same\" classification distribution (using sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold). \n",
    "\n",
    "In a first time, I used those Methods to extract folds (##TODO : see if I have the time to implement it myself) and I computed the Cross Validation, to find the best hyper-parameters (in terms of acuracy) of my KNN.\n",
    "\n",
    "\n",
    "## 1. Step by step construction :\n",
    "\n",
    "This first part is decomposed step by step, then I agglomerate, and then I pass it into a sub class of scikit-learn \"BaseCrossValidator\" class\n",
    "\n",
    "### 1.1 Initialization and preprocess\n",
    "\n",
    "First, I initialize my parameters and define how to split and standardize my data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regional-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ DATA : \n",
    "df = pd.read_csv(data_path+'winequality-red+.csv', sep=\";\")\n",
    "\n",
    "## DESIGN AND SAMPLING : \n",
    "X = df.drop(\"quality\", axis = 1, inplace = False)\n",
    "threshold = np.floor(df.quality.mean())\n",
    "y = np.where(df.quality < threshold, 0,1) \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "## SET ARGUMENTS/PARAMETERS :\n",
    "clf_meth = neighbors.KNeighborsClassifier()\n",
    "param_grid = {\"n_neighbors\" : np.arange(2,15,1)}\n",
    "cv = 5\n",
    "\n",
    "## INITIALIZATION OF DATA FOR CV: \n",
    "CV_X = X_train\n",
    "CV_y = y_train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-suite",
   "metadata": {},
   "source": [
    "As the standardization will be needed for each fold, let's implement it in a function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "warming-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTRACT X_train, X_test FROM X AND train_index :    \n",
    "def extract_Xsplitted_data(X, train_index):    \n",
    "#     train_index = X_train.index\n",
    "    test_index = [(index not in train_index) for index in X.index]\n",
    "    X_train = X.loc[train_index]\n",
    "    X_test = X.loc[test_index]\n",
    "    return(X_train, X_test)\n",
    "\n",
    "## STANDARDIZATION : \n",
    "def normalize_from_split(X, train_index):\n",
    "    X_train, X_test = extract_Xsplitted_data(X, train_index)\n",
    "    \n",
    "    my_normalizer = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_std = pd.DataFrame(my_normalizer.transform(X_train), index = X_train.index, columns = X_train.columns)\n",
    "    X_test_std = pd.DataFrame(my_normalizer.transform(X_test), index = X_test.index, columns = X_test.columns)\n",
    "\n",
    "    return(X_train_std, X_test_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-agreement",
   "metadata": {},
   "source": [
    "### 1.2 Define KNN method and loop iterators (folds and kwargs)\n",
    "Secondly, I needed to separate into folds. To do so, I used the \"naive\" KFold method implemented in sklearn, and its improvement that take into account the repartition of y classes, \"StratifiedKFold\" : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acoustic-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits = 5)\n",
    "# CV_split_iterator = kf.split(CV_X)\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "CV_split_iterator = kf.split(CV_X, y = CV_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-blogger",
   "metadata": {},
   "source": [
    "Then, I call the functions defined above to extract the train and test samples and standardize with X_train data, and loop on the different folds splits : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coordinated-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### LOOP ON FOLDS :\n",
    "for CV_train_range_index, CV_test_range_index in CV_split_iterator : \n",
    "    train_index = CV_X.index[CV_train_range_index]\n",
    "    test_index = CV_X.index[CV_test_range_index] \n",
    "    \n",
    "    ## NORMALIZE :\n",
    "    CV_X_train, CV_X_test = normalize_from_split(CV_X, train_index)\n",
    "    ## GET y SPLIT : \n",
    "    CV_y_train, CV_y_test = y[train_index], y[test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-composer",
   "metadata": {},
   "source": [
    "And within each iteration, I'll need to loop again, on the parameters of the KNN models, contained in \"param_grid\". To do so, the idea is to transform the dictionnary of lists into a list of dictionnary, mapping every possible values. The \"model_selection.ParameterGrid()\" exactly does that : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP THE DICT OF LIST INTO LIST OF DICT :\n",
    "param_grid = {\"a\":{0,1,2}, \"b\":{3,4}}\n",
    "param_dirg = model_selection.ParameterGrid(param_grid)\n",
    "for kwargs in param_dirg :\n",
    "    print(kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-print",
   "metadata": {},
   "source": [
    "And I used the \"kwargs\" syntax to call the dictionary of parameters in KNN model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"n_neighbors\":3, 'metric': 'euclidean'}\n",
    "\n",
    "knn_clf = neighbors.KNeighborsClassifier(**param)\n",
    "knn_clf.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-pickup",
   "metadata": {},
   "source": [
    "## 2. Cross validation :\n",
    "\n",
    "### 2.1 Code summary\n",
    "\n",
    "Ok, so, the essential code to launch the CV is the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caring-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESIGN AND SAMPLING : \n",
    "X = df.drop(\"quality\", axis = 1, inplace = False)\n",
    "threshold = np.floor(df.quality.mean())\n",
    "y = np.where(df.quality < threshold, 0,1) \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "## SET ARGUMENTS/PARAMETERS :\n",
    "clf_meth = neighbors.KNeighborsClassifier()\n",
    "# param_grid = {\"n_neighbors\" : np.arange(2,15,1)}\n",
    "param_grid = {\"n_neighbors\" : np.arange(2,15,1),\"metric\": {'euclidean', 'manhattan'}}\n",
    "\n",
    "cv = 5\n",
    "\n",
    "###############################################################\n",
    "##TODO : toutes les variables \"locales\" sont préfixées de 'CV_' \n",
    "\n",
    "## INITIALIZATION : \n",
    "CV_X = X_train\n",
    "CV_y = y_train \n",
    "\n",
    "## SET FOLDS : \n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "CV_split_iterator = kf.split(CV_X, y = CV_y)\n",
    "## MAP THE DICT OF LIST INTO LIST OF DICT :\n",
    "param_dirg = model_selection.ParameterGrid(param_grid)\n",
    "\n",
    "### LOOP ON FOLDS :\n",
    "for CV_train_range_index, CV_test_range_index in CV_split_iterator : \n",
    "    train_index = CV_X.index[CV_train_range_index]\n",
    "    test_index = CV_X.index[CV_test_range_index] \n",
    "    \n",
    "    ## NORMALIZE :\n",
    "    CV_X_train, CV_X_test = normalize_from_split(CV_X, train_index)\n",
    "    ## GET y SPLIT : \n",
    "    CV_y_train, CV_y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    ### LOOP ON PARAM NAMES (HERE ONLY 1)\n",
    "    for kwargs in param_dirg :\n",
    "        knn_clf = neighbors.KNeighborsClassifier(**kwargs)\n",
    "        ## FIT KNN on X_train AND PREDICT ON X_test :\n",
    "        knn_clf.fit(CV_X_train,CV_y_train)\n",
    "        \n",
    "        accuracy = knn_clf.score(CV_X_test, CV_y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-hayes",
   "metadata": {},
   "source": [
    "### 2.2 Save results in the procedure : \n",
    "But our aim is to extract some information from it. As an example, the sklearn method I aim to mimique as an attribute \"CV_results_\", with the keys : \n",
    "\n",
    "           'mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', \n",
    "           'param_metric', 'param_n_neighbors', 'params', 'split0_test_score',\n",
    "           'split1_test_score', 'split2_test_score', 'split3_test_score',\n",
    "           'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'\n",
    "and as an example, with a \n",
    "it returns : \n",
    "        \n",
    "     {'mean_fit_time': array([0.00328555, 0.00298281, 0.00299325, 0.0029798 ]),\n",
    "     'std_fit_time': array([3.45699297e-04, 1.59457825e-04, 9.06425422e-05, 4.99484135e-05]),\n",
    "     'mean_score_time': array([0.01436076, 0.01277046, 0.01385365, 0.01308317]),\n",
    "     'std_score_time': array([0.00377103, 0.00024649, 0.00123275, 0.00020375]),\n",
    "     'param_metric': masked_array(data=['euclidean', 'euclidean', 'manhattan', 'manhattan'],\n",
    "                                  mask=[False, False, False, False],\n",
    "                                  fill_value='?', dtype=object),\n",
    "     'param_n_neighbors': masked_array(data=[10, 15, 10, 15],\n",
    "                                       mask=[False, False, False, False],\n",
    "                                       fill_value='?', dtype=object),\n",
    "     'params':[{'metric': 'euclidean', 'n_neighbors': 10},\n",
    "               {'metric': 'euclidean', 'n_neighbors': 15},\n",
    "               {'metric': 'manhattan', 'n_neighbors': 10},\n",
    "               {'metric': 'manhattan', 'n_neighbors': 15}],\n",
    "     'split0_test_score': array([0.96875   , 0.96875   , 0.96428571, 0.96875   ]),\n",
    "     'split1_test_score': array([0.96428571, 0.96428571, 0.96428571, 0.96428571]),\n",
    "     'split2_test_score': array([0.96428571, 0.96428571, 0.95982143, 0.96428571]),\n",
    "     'split3_test_score': array([0.96428571, 0.96428571, 0.96428571, 0.96428571]),\n",
    "     'split4_test_score': array([0.96860987, 0.96860987, 0.96412556, 0.96860987]),\n",
    "     'mean_test_score': array([0.9660434 , 0.9660434 , 0.96336083, 0.9660434 ]),\n",
    "     'std_test_score': array([0.00215317, 0.00215317, 0.00177079, 0.00215317]),\n",
    "     'rank_test_score': array([1, 1, 4, 1], dtype=int32)}\n",
    "     \n",
    "But I can first just extract the accuracy mean (that is what is expected !) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "recovered-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DESIGN AND SAMPLING : \n",
    "X = df.drop(\"quality\", axis = 1, inplace = False)\n",
    "threshold = np.floor(df.quality.mean())\n",
    "y = np.where(df.quality < threshold, 0,1) \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "## SET ARGUMENTS/PARAMETERS :\n",
    "clf_meth = neighbors.KNeighborsClassifier()\n",
    "# NUMBER OF FOLDS :\n",
    "cv = 5\n",
    "param_grid = {\"n_neighbors\" : np.arange(2,15,1)}\n",
    "# param_grid = {\"n_neighbors\" : np.arange(2,15,1),\"metric\": {'euclidean', 'manhattan'}}\n",
    "# MAP THE DICT OF LIST INTO LIST OF DICT :\n",
    "param_dirg = model_selection.ParameterGrid(param_grid)\n",
    "\n",
    "###############################################################\n",
    "##TODO : toutes les variables \"locales\" sont préfixées de 'CV_' \n",
    "\n",
    "## INITIALIZATION : \n",
    "CV_X = X_train\n",
    "CV_y = y_train \n",
    "\n",
    "index = [(\"fold\"+str(k+1)) for k in range(cv)]\n",
    "k=0\n",
    "columns = [(\"_\".join(str(val) for val in kwargs.values())) for kwargs in param_dirg]\n",
    "accuracy_df = pd.DataFrame(index = index, columns = columns )\n",
    "\n",
    "## SET FOLDS : \n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "CV_split_iterator = kf.split(CV_X, y = CV_y)\n",
    "\n",
    "### LOOP ON FOLDS :\n",
    "for CV_train_range_index, CV_test_range_index in CV_split_iterator : \n",
    "    k+=1\n",
    "    index_name = \"fold\"+str(k)\n",
    "    train_index = CV_X.index[CV_train_range_index]\n",
    "    test_index = CV_X.index[CV_test_range_index] \n",
    "    \n",
    "    ## NORMALIZE :\n",
    "    CV_X_train, CV_X_test = normalize_from_split(CV_X, train_index)\n",
    "    ## GET y SPLIT : \n",
    "    CV_y_train, CV_y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    ### LOOP ON PARAM NAMES (HERE ONLY 1)\n",
    "    for kwargs in param_dirg :\n",
    "        ## FIT KNN on X_train AND PREDICT ON X_test :\n",
    "        knn_clf = neighbors.KNeighborsClassifier(**kwargs)\n",
    "        knn_clf.fit(CV_X_train,CV_y_train)\n",
    "        \n",
    "        \n",
    "        column_name = \"_\".join(str(val) for val in kwargs.values())\n",
    "        accuracy_df.loc[index_name, column_name] = knn_clf.score(CV_X_test, CV_y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "involved-charter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('manhattan_2', 0.9276145099295323, 0.016215779492600276)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df.columns[0], accuracy_df.mean(axis = 0)[0], accuracy_df.std(axis = 0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-carbon",
   "metadata": {},
   "source": [
    "Now, the graal would have been to create a results class, that contains the different dataFrames such that accuracy_df, but also time ... that would be filled given a index_name (=fold) and column_name (=param grid product) ... See if I have time to do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-perth",
   "metadata": {},
   "source": [
    "# 3. TP \" Sélectionnez le nombre de voisins dans un kNN\" with my CV : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "frequent-electricity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.923 (+/-0.035) for n_neighbors = 2\n",
      "accuracy = 0.959 (+/-0.015) for n_neighbors = 3\n",
      "accuracy = 0.956 (+/-0.023) for n_neighbors = 4\n",
      "accuracy = 0.962 (+/-0.010) for n_neighbors = 5\n",
      "accuracy = 0.961 (+/-0.017) for n_neighbors = 6\n",
      "accuracy = 0.963 (+/-0.015) for n_neighbors = 7\n",
      "accuracy = 0.963 (+/-0.015) for n_neighbors = 8\n",
      "accuracy = 0.965 (+/-0.007) for n_neighbors = 9\n",
      "accuracy = 0.962 (+/-0.010) for n_neighbors = 10\n",
      "accuracy = 0.966 (+/-0.005) for n_neighbors = 11\n",
      "accuracy = 0.965 (+/-0.007) for n_neighbors = 12\n",
      "accuracy = 0.965 (+/-0.007) for n_neighbors = 13\n",
      "accuracy = 0.965 (+/-0.007) for n_neighbors = 14\n"
     ]
    }
   ],
   "source": [
    "data = X_train, y_train \n",
    "## SET ARGUMENTS/PARAMETERS :\n",
    "clf_meth = neighbors.KNeighborsClassifier()\n",
    "# NUMBER OF FOLDS :\n",
    "cv = 5\n",
    "param_grid = {\"n_neighbors\" : np.arange(2,15,1)}\n",
    "# param_grid = {\"n_neighbors\" : np.arange(2,15,1),\"metric\": {'euclidean', 'manhattan'}}\n",
    "\n",
    "def my_CV(data, param_grid, clf_meth) :\n",
    "    # MAP THE DICT OF LIST INTO LIST OF DICT :\n",
    "    param_dirg = model_selection.ParameterGrid(param_grid)\n",
    "\n",
    "    ###############################################################\n",
    "    ##TODO : toutes les variables \"locales\" sont préfixées de 'CV_' \n",
    "\n",
    "    ## INITIALIZATION : \n",
    "    CV_X, CV_y = data \n",
    "#     CV_X = X_train\n",
    "#     CV_y = y_train \n",
    "\n",
    "    index = [(\"fold\"+str(k+1)) for k in range(cv)]\n",
    "    k=0\n",
    "    columns = [(\"_\".join(str(val) for val in kwargs.values())) for kwargs in param_dirg]\n",
    "    accuracy_df = pd.DataFrame(index = index, columns = columns )\n",
    "\n",
    "    ## SET FOLDS : \n",
    "    kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "    CV_split_iterator = kf.split(CV_X, y = CV_y)\n",
    "\n",
    "    ### LOOP ON FOLDS :\n",
    "    for CV_train_range_index, CV_test_range_index in CV_split_iterator : \n",
    "        k+=1\n",
    "        index_name = \"fold\"+str(k)\n",
    "        train_index = CV_X.index[CV_train_range_index]\n",
    "        test_index = CV_X.index[CV_test_range_index] \n",
    "\n",
    "        ## NORMALIZE :\n",
    "        CV_X_train, CV_X_test = normalize_from_split(CV_X, train_index)\n",
    "        ## GET y SPLIT : \n",
    "        CV_y_train, CV_y_test = y[train_index], y[test_index]\n",
    "\n",
    "        ### LOOP ON PARAM NAMES (HERE ONLY 1)\n",
    "        for kwargs in param_dirg :\n",
    "            ## FIT KNN on X_train AND PREDICT ON X_test :\n",
    "            knn_clf = neighbors.KNeighborsClassifier(**kwargs)\n",
    "            knn_clf.fit(CV_X_train,CV_y_train)\n",
    "\n",
    "\n",
    "            column_name = \"_\".join(str(val) for val in kwargs.values())\n",
    "            accuracy_df.loc[index_name, column_name] = knn_clf.score(CV_X_test, CV_y_test)\n",
    "            \n",
    "    return pd.DataFrame((accuracy_df.columns, accuracy_df.mean(axis = 0), accuracy_df.std(axis = 0)), index = ['n_neighbors', 'mean','std'])\n",
    "\n",
    "res= my_CV(data, param_grid, clf_meth)\n",
    "\n",
    "for col_index in res.columns:\n",
    "    mean = res.loc[\"mean\",col_index]\n",
    "    std = res.loc[\"std\",col_index]\n",
    "    params = res.loc[\"n_neighbors\",col_index]\n",
    "    print(\"accuracy = %0.3f (+/-%0.3f) for n_neighbors = %s\" %(mean, 2*std, params))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
