{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graphic-netscape",
   "metadata": {},
   "source": [
    "# TP Chapitre - Entraînez-vous : implémentez une validation croisée\n",
    "\n",
    "This the notebook where I constructed the method, step by step, so there are probably repeated parts of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adopted-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"~/Documents/openclassroom/Fomation_ingenieur_ML/data/\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## sklearn module : \n",
    "from sklearn import model_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ranking-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+'winequality-red+.csv', sep=\";\")\n",
    "\n",
    "## DESIGN AND SAMPLING : \n",
    "X = df.drop(\"quality\", axis = 1, inplace = False)\n",
    "threshold = np.floor(df.quality.mean())\n",
    "y = np.where(df.quality < threshold, 0,1) \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "#### comment je créée mes conteneurs pour extraire les données train/test ?\n",
    "## PAR UN DICT GLOBAL ?\n",
    "# original_data_split={\"X_train\":X_train,\"X_test\":X_test,\n",
    "#                      \"y_train\":y_train, \"y_test\":y_test}\n",
    "# original_data_split[\"X_train\"]\n",
    "\n",
    "# class DataSplit:\n",
    "#    def __init__(self):\n",
    "#     X_train = self.X_train\n",
    "#     y_train = self.y_train\n",
    "#     X_test = self.X_test\n",
    "#     y_test = self.y_test\n",
    "    \n",
    "    \n",
    "## EXTRACT X_train, X_test FROM X AND train_index :    \n",
    "def extract_Xsplitted_data(X, train_index):    \n",
    "#     train_index = X_train.index\n",
    "    test_index = [(index not in train_index) for index in X.index]\n",
    "    X_train = X.loc[train_index]\n",
    "    X_test = X.loc[test_index]\n",
    "    return(X_train, X_test)\n",
    "\n",
    "## STANDARDIZATION : \n",
    "def normalize_from_split(X, train_index):\n",
    "    X_train, X_test = extract_Xsplitted_data(X, train_index)\n",
    "    \n",
    "    my_normalizer = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_std = pd.DataFrame(my_normalizer.transform(X_train), index = X_train.index, columns = X_train.columns)\n",
    "    X_test_std = pd.DataFrame(my_normalizer.transform(X_test), index = X_test.index, columns = X_test.columns)\n",
    "\n",
    "    return(X_train_std, X_test_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-grain",
   "metadata": {},
   "source": [
    "###### LIST OF ARGUMENTS AT THE END \n",
    "clf_meth = neighbors.KNeighborsClassifier()\n",
    "\n",
    "param_grid = {\"n_neighbors\" : np.arange(2,15,1)}\n",
    "\n",
    "cv = 5\n",
    "\n",
    "###### WHAT IT DOES : \n",
    "1. Create \"cv\"(=5 for ex) folds \n",
    "    \n",
    "    1.1 Separate the observations in 5 parts that respect the global distrib (folds)\n",
    "\n",
    "    1.2 Loop for fold_f in 1:5, \n",
    "            fit the clf_meth model on the Xcv_train = X_train[all fold - fold_f],\n",
    "    \n",
    "            loop for params in param_grid.keys() : \n",
    "            \n",
    "                fit clf_meth(Xcv_train, ycv_train\n",
    "            \n",
    "            save results for prediction on Xcv_test = X_train[fold_f] \n",
    "\n",
    "2. define class methods : \n",
    "    \n",
    "    2.1 cv_results_ : save mean results of models in the dict param_grid.key()\n",
    "    \n",
    "    2.2 best_params_ : the better parameter set in param_grid, in term of accuracy\n",
    "    ... many other methods, see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-table",
   "metadata": {},
   "source": [
    "## 1. With the Kflod function in scikit-learn :\n",
    "In the chapter \"Mettez en place un cadre de validation croisée\", they explain all the approach of the Cross Validation, from the separation in kflod, with the \"same\" classification distribution (using sklearn.model_selection.KFold or sklearn.model_selection.StratifiedKFold). So, let's first implement the second part, that is where the hyperparameters are fitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "caring-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neighbors\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "n_neighbors\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "n_neighbors\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "n_neighbors\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "n_neighbors\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# ## DESIGN AND SAMPLING : \n",
    "# X = df.drop(\"quality\", axis = 1, inplace = False)\n",
    "# threshold = np.floor(df.quality.mean())\n",
    "# y = np.where(df.quality < threshold, 0,1) \n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "\n",
    "clf_meth = neighbors.KNeighborsClassifier()\n",
    "param_grid = {\"n_neighbors\" : np.arange(2,15,1)}\n",
    "cv = 5\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "##TODO : toutes les variables \"locales\" sont préfixées de 'CV_' \n",
    "CV_X = X_train\n",
    "CV_y = y_train \n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "# kf.get_n_splits(argX)\n",
    "\n",
    "for CV_train_range_index, CV_test_range_index in kf.split(CV_X) : \n",
    "    train_index = CV_X.index[train_range_index]\n",
    "    test_index = CV_X.index[test_range_index] \n",
    "    \n",
    "    ## NORMALIZE :\n",
    "    CV_X_train, CV_X_test = normalize_from_split(CV_X, train_index)\n",
    "    ## GET y SPLIT : \n",
    "    CV_y_train, CV_y_test = y[train_index], y[test_index]\n",
    "    \n",
    "#     ## LOOP ON PARAM NAMES (HERE ONLY 1)\n",
    "#     for param_name in param_grid.keys() :\n",
    "    param_name =  next(iter(param_grid))\n",
    "\n",
    "    print(param_name)\n",
    "    for param_value in param_grid[param_name]:\n",
    "        print(param_value)\n",
    "    ## FIT KNN on X_train AND PREDICT ON X_test :\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "informative-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'n_neighbors': 2}\n",
      "{'algorithm': 'auto', 'n_neighbors': 3}\n",
      "{'algorithm': 'auto', 'n_neighbors': 4}\n",
      "{'algorithm': 'auto', 'n_neighbors': 5}\n",
      "{'algorithm': 'auto', 'n_neighbors': 6}\n",
      "{'algorithm': 'auto', 'n_neighbors': 7}\n",
      "{'algorithm': 'auto', 'n_neighbors': 8}\n",
      "{'algorithm': 'auto', 'n_neighbors': 9}\n",
      "{'algorithm': 'auto', 'n_neighbors': 10}\n",
      "{'algorithm': 'auto', 'n_neighbors': 11}\n",
      "{'algorithm': 'auto', 'n_neighbors': 12}\n",
      "{'algorithm': 'auto', 'n_neighbors': 13}\n",
      "{'algorithm': 'auto', 'n_neighbors': 14}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 2}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 3}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 4}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 5}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 6}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 7}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 8}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 9}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 10}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 11}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 12}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 13}\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 14}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 2}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 3}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 4}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 5}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 6}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 7}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 8}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 9}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 10}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 11}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 12}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 13}\n",
      "{'algorithm': 'kd_tree', 'n_neighbors': 14}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"n_neighbors\" : np.arange(2,15,1), \n",
    "              \"algorithm\" : [\"auto\", \"ball_tree\", \"kd_tree\"] }\n",
    "\n",
    "param_dirg = [dict(zip(param_grid, val)) for val in zip(*param_grid.values())]\n",
    "# param_name =  next(iter(param_grid))\n",
    "\n",
    "# for key in iter(param_grid):\n",
    "#     for val in param_grid[key]:\n",
    "#         print(key, val)\n",
    "        \n",
    "# my_list = []\n",
    "# for key in param_grid.key():\n",
    "#     my_dict = {}\n",
    "#     for val in param_grid[key]: \n",
    "#         my_dict[key] = val\n",
    "        \n",
    "param_grid.items()\n",
    "\n",
    "param_dirg = model_selection.ParameterGrid(param_grid)\n",
    "for my_dict in param_dirg :\n",
    "    print(my_dict)\n",
    "\n",
    "#     for val in param_grid[key]:\n",
    "#         print(key, val)\n",
    "        \n",
    "#     print(key)\n",
    "# print(iter(param_grid))\n",
    "# for param_value in param_grid[param_name]:\n",
    "#     print(param_value)\n",
    "# #         knn_clf = neighbors.KNeighborsClassifier(param_name=param_value)\n",
    "# #         knn_clf.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "alien-abortion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_name = \"n_neighbors\"\n",
    "param_value = 3\n",
    "\n",
    "param = {}\n",
    "\n",
    "knn_clf = neighbors.KNeighborsClassifier(**param)\n",
    "\n",
    "knn_clf.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-onion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "threatened-worship",
   "metadata": {},
   "source": [
    "Then I went to see the scikit-learn class defined for cross validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCrossValidator(metaclass=ABCMeta):\n",
    "    \"\"\"Base class for all cross-validators\n",
    "    Implementations must define `_iter_test_masks` or `_iter_test_indices`.\n",
    "    \"\"\"\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target variable for supervised learning problems.\n",
    "        groups : array-like of shape (n_samples,), default=None\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        indices = np.arange(_num_samples(X))\n",
    "        for test_index in self._iter_test_masks(X, y, groups):\n",
    "            train_index = indices[np.logical_not(test_index)]\n",
    "            test_index = indices[test_index]\n",
    "            yield train_index, test_index\n",
    "\n",
    "    # Since subclasses must implement either _iter_test_masks or\n",
    "    # _iter_test_indices, neither can be abstract.\n",
    "    def _iter_test_masks(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Generates boolean masks corresponding to test sets.\n",
    "        By default, delegates to _iter_test_indices(X, y, groups)\n",
    "        \"\"\"\n",
    "        for test_index in self._iter_test_indices(X, y, groups):\n",
    "            test_mask = np.zeros(_num_samples(X), dtype=bool)\n",
    "            test_mask[test_index] = True\n",
    "            yield test_mask\n",
    "\n",
    "    def _iter_test_indices(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Generates integer indices corresponding to test sets.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        \"\"\"Returns the number of splitting iterations in the cross-validator\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return _build_repr(self)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
