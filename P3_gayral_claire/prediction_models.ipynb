{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e313b469",
   "metadata": {},
   "source": [
    "# Modèles de prédiction \n",
    "\n",
    "Je propose de résumer ici le travail d'exploration fait dans le premier notebook. \n",
    "\n",
    "L'objectif est de prédire deux variables numériques : la consommation d'énergie des bâtiments ```SiteEnergyUse(kBtu)```et la quantité d'émission de CO2 ```CO2_emissions```. Après différents pré-traitements tels que l'élimination des variables corrélées entres elles, ou la vectorisation des variables catégorielles, le jeu d'entrainement a la forme suivante : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826cb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_path = \"/home/clairegayral/Documents/openclassroom/data/P3/\"\n",
    "res_path = \"/home/clairegayral/Documents/openclassroom/res/P3/\"\n",
    "\n",
    "## my .py : \n",
    "from P3_preprocess import *\n",
    "import missing_values_treatment\n",
    "import duplicates\n",
    "import univariate_analysis\n",
    "from multivariate_analysis import *\n",
    "\n",
    "from sklearn import model_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2aa235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 training numerical variables :  ['ZipCode', 'NumberofFloors', 'PropertyGFAParking', 'LargestPropertyUseTypeGFA', 'ENERGYSTARScore', 'SiteEnergyUse(kBtu)', 'CO2_emissions', 'age_of_building', 'SPD Beats']\n",
      "23 training interger variables (from tokenization of categorical variable)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>NumberofFloors</th>\n",
       "      <th>PropertyGFAParking</th>\n",
       "      <th>LargestPropertyUseTypeGFA</th>\n",
       "      <th>ENERGYSTARScore</th>\n",
       "      <th>SiteEnergyUse(kBtu)</th>\n",
       "      <th>CO2_emissions</th>\n",
       "      <th>age_of_building</th>\n",
       "      <th>SPD Beats</th>\n",
       "      <th>BuildingType_campus</th>\n",
       "      <th>...</th>\n",
       "      <th>LargestPropertyUseType_data_center</th>\n",
       "      <th>LargestPropertyUseType_side_medical</th>\n",
       "      <th>LargestPropertyUseType_1</th>\n",
       "      <th>LargestPropertyUseType_2</th>\n",
       "      <th>LargestPropertyUseType_3</th>\n",
       "      <th>LargestPropertyUseType_4</th>\n",
       "      <th>LargestPropertyUseType_5</th>\n",
       "      <th>LargestPropertyUseType_6</th>\n",
       "      <th>LargestPropertyUseType_7</th>\n",
       "      <th>LargestPropertyUseType_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98101.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88434.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>7103895.25</td>\n",
       "      <td>249.705</td>\n",
       "      <td>88.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98101.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15064.0</td>\n",
       "      <td>83880.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>8371084.00</td>\n",
       "      <td>279.685</td>\n",
       "      <td>19.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>98359.0</td>\n",
       "      <td>756868.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>72858840.00</td>\n",
       "      <td>2075.380</td>\n",
       "      <td>46.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98101.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61320.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>17511952.00</td>\n",
       "      <td>1111.385</td>\n",
       "      <td>89.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98121.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37230.0</td>\n",
       "      <td>123445.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>14500852.50</td>\n",
       "      <td>506.355</td>\n",
       "      <td>35.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ZipCode  NumberofFloors  PropertyGFAParking  LargestPropertyUseTypeGFA  \\\n",
       "0  98101.0            12.0                 0.0                    88434.0   \n",
       "1  98101.0            11.0             15064.0                    83880.0   \n",
       "2  98101.0            41.0             98359.0                   756868.0   \n",
       "3  98101.0            10.0                 0.0                    61320.0   \n",
       "4  98121.0            18.0             37230.0                   123445.0   \n",
       "\n",
       "   ENERGYSTARScore  SiteEnergyUse(kBtu)  CO2_emissions  age_of_building  \\\n",
       "0             62.5           7103895.25        249.705             88.5   \n",
       "1             56.0           8371084.00        279.685             19.5   \n",
       "2             30.5          72858840.00       2075.380             46.5   \n",
       "3             28.5          17511952.00       1111.385             89.5   \n",
       "4             71.0          14500852.50        506.355             35.5   \n",
       "\n",
       "   SPD Beats  BuildingType_campus  ...  LargestPropertyUseType_data_center  \\\n",
       "0       31.0                    0  ...                                   0   \n",
       "1       31.0                    0  ...                                   0   \n",
       "2       31.0                    0  ...                                   0   \n",
       "3       31.0                    0  ...                                   0   \n",
       "4       31.0                    0  ...                                   0   \n",
       "\n",
       "   LargestPropertyUseType_side_medical  LargestPropertyUseType_1  \\\n",
       "0                                    0                         1   \n",
       "1                                    0                         1   \n",
       "2                                    0                         1   \n",
       "3                                    0                         1   \n",
       "4                                    0                         1   \n",
       "\n",
       "   LargestPropertyUseType_2  LargestPropertyUseType_3  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   LargestPropertyUseType_4  LargestPropertyUseType_5  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   LargestPropertyUseType_6  LargestPropertyUseType_7  \\\n",
       "0                         0                         0   \n",
       "1                         0                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         0                         0   \n",
       "\n",
       "   LargestPropertyUseType_8  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(res_path+\"cleanned_data.csv\", index_col=0)\n",
    "print(len(data.columns[data.dtypes==float])-3, \"training numerical variables : \",\n",
    "      list(data.columns[data.dtypes==float]))\n",
    "print(len(data.columns[data.dtypes==int]), \n",
    "      \"training interger variables (from tokenization of categorical variable)\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae668c7",
   "metadata": {},
   "source": [
    "# Mise à l'écart de l'ensemble de test : \n",
    "Je mets de côté l'ensemble de test, sur lequel je comparerai mes modèles prédictifs à la fin. \n",
    "\n",
    "### Séparation données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc46b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_original = data.drop([\"SiteEnergyUse(kBtu)\",\"CO2_emissions\", \"ENERGYSTARScore\"], axis = 1)\n",
    "y_original = data[[\"SiteEnergyUse(kBtu)\",\"CO2_emissions\"]]\n",
    "\n",
    "X_original_train, X_original_test, y_original_train, y_original_test = train_test_split(X_original, y_original, train_size=0.8)\n",
    "\n",
    "index_train = y_original_train.index\n",
    "index_test = y_original_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a4069",
   "metadata": {},
   "source": [
    "Par soucis de clarté de code, je vais renommer $X\\_original\\_train$ avec $X$. **Ainsi, le lecteur pourra bien comprendre que le $X$ évoqué dans la suite sera bien l'ensemble d'entrainement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79f4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_original_train\n",
    "y = y_original_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8296676",
   "metadata": {},
   "source": [
    "### Standardisation entrainée sur l'ensemble d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f5862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "my_standardizer = preprocessing.StandardScaler()\n",
    "my_standardizer.fit(X)\n",
    "\n",
    "## standardization \n",
    "my_standardizer.fit(X)\n",
    "X_std = pd.DataFrame(my_standardizer.transform(X), columns=X.columns, index = X.index)\n",
    "X_original_test_std = my_standardizer.transform(X_original_test)\n",
    "\n",
    "## std y ?\n",
    "my_standardizer.fit(y_original_train)\n",
    "\n",
    "y_std = pd.DataFrame(my_standardizer.transform(y), columns = y.columns, index = y.index)\n",
    "y_original_test_std = my_standardizer.transform(y_original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d88eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## log-transform :\n",
    "def df_log_transform(X_):\n",
    "    num_var = X_.columns[X_.dtypes==float].values\n",
    "    X_log = X_.copy()\n",
    "    ## replace neg values with 0 : \n",
    "    index_negative = np.where(X_log<0)\n",
    "    X_log.iloc[[ind for ind in index_negative]] = 0 \n",
    "    X_log.at[:,num_var] = np.log(X_log[num_var]+1)\n",
    "    return(X_log)\n",
    "\n",
    "X_log = df_log_transform(X)\n",
    "\n",
    "## standardization of log-transfo :\n",
    "my_standardizer.fit(X_log)\n",
    "X_log_std = pd.DataFrame(my_standardizer.transform(X_log), \n",
    "                         columns=X.columns, index = X.index)\n",
    "X_original_test_log = df_log_transform(X_original_test)\n",
    "X_original_test_log_std = pd.DataFrame(my_standardizer.transform(X_original_test_log),\n",
    "                                       columns=X_original_test.columns, index = X_original_test.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a465bd",
   "metadata": {},
   "source": [
    "# Régression Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for SiteEnergyUse(kBtu) in log space :\n"
     ]
    }
   ],
   "source": [
    "dict_param_grid = {\"ridge\": np.logspace(-1, 4, 25),\n",
    "                   \"lasso\": np.logspace(-2, 2, 25),\n",
    "                   \"enet\" : np.logspace(-2, 2, 25),\n",
    "                  }\n",
    "\n",
    "dict_models = {\"ridge\" : linear_model.Ridge(), \n",
    "               \"lasso\" : linear_model.Lasso(),\n",
    "               \"enet\" : linear_model.ElasticNet(),\n",
    "              }\n",
    "\n",
    "var = \"SiteEnergyUse(kBtu)\"\n",
    "X_ = X_log_std\n",
    "y_ = np.log(y_std[var]+1)\n",
    "print(\"for\", var, \"in log space :\")\n",
    "res = compare_regressions(X_, y_, dict_models, dict_param_grid, score_name=\"r2\", fig_name=None)\n",
    "plt.show()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f01577",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"CO2_emissions\"\n",
    "X_ = X_log_std\n",
    "y_ = np.log(y_std[var]+1)\n",
    "print(\"for\", var, \"in log space :\")\n",
    "res = compare_regressions(X_, y_, dict_models, dict_param_grid, score_name=\"r2\", fig_name=None)\n",
    "plt.show()\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d383b1",
   "metadata": {},
   "source": [
    "La régression linéaire, même avec pénalisation, ne semble pas produire un modèle de prédiction convainquant, notamment à cause de la lourde queue de distribution des variables à prédire. Néansmoins, je vais utiliser la régression pénalisée lasso sur les variables \"catégorielles\" (issues de la vectorisation de ces dernière pour être précise,  c'est à dire faire une ANOVA), afin de voir si certaines de mes modalités sont non significatives (et peuvent donc être retirées)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5136d2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_param_grid[\"lasso\"] = np.logspace(-4.5, 0, 25)\n",
    "\n",
    "var = \"SiteEnergyUse(kBtu)\"\n",
    "X_ = X.loc[:,X.dtypes == int]\n",
    "y_ = np.log(y_std[var]+1)\n",
    "\n",
    "print(\"for\", var, \"in log space :\")\n",
    "res = compare_regressions(X_, y_, {\"lasso\" : linear_model.Lasso(max_iter=10000)}, \n",
    "                          dict_param_grid, score_name=\"r2\", fig_name=None)\n",
    "plt.show()\n",
    "## regul paths :\n",
    "model_name = \"lasso\"\n",
    "legend_kwargs = {\"loc\" : \"upper right\",\"bbox_to_anchor\":(2.5, 1), \"ncol\":2}\n",
    "plot_regul_paths(alpha_values = dict_param_grid[model_name], lm_model = dict_models[model_name], \n",
    "                 X_ = X_, y_ = y_, best_alpha=res.loc[model_name, \"best_alpha\"],\n",
    "                 var_names = X_.columns, legend_kwargs=legend_kwargs)\n",
    "plt.show()\n",
    "\n",
    "model = dict_models[model_name]\n",
    "model.set_params(alpha = res.loc[model_name, \"best_alpha\"])\n",
    "model.fit(X_,y_)\n",
    "models_coefs = model.coef_\n",
    "print(\"Modalities with zero coefficient in ANOVA :\")\n",
    "zero_coeff_modality = X_.columns[[abs(coef) < 10**(-5) for coef in models_coefs]]\n",
    "zero_coeff_modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85e752",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_.columns[[abs(coef) >= 10**(-5) for coef in models_coefs]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa21f48",
   "metadata": {},
   "source": [
    "Je vais donc retirer ces méta-modalités, car elle n'ont pas d'influence dans cette ANOVA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5991e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(zero_coeff_modality.values, axis =1)\n",
    "X_log = X_log.drop(zero_coeff_modality.values, axis =1)\n",
    "\n",
    "X_std = X_std.drop(zero_coeff_modality.values, axis = 1)\n",
    "X_log_std = X_log_std.drop(zero_coeff_modality.values, axis =1)\n",
    "X_original_test_log_std = X_original_test_log_std.drop(zero_coeff_modality.values, axis =1)\n",
    "# X_std = pd.DataFrame(my_standardizer.fit_transform(X), columns=X.columns, index = X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a10fff",
   "metadata": {},
   "source": [
    "# Liste des modèles testés : \n",
    "\n",
    "Pour chacun des modèles de régression testé, j'ai construit un dictionnaire des arguments par défaut (```dict_kwargs_models```). Ce dictionnaire a les mêmes clés \"modèles_de_regression\" que les dictionnaires ```dict_models``` et ```dict_param_grid```, correspondant respectivement au dictionnaire des fonctions de regression (principalement de sklearn) et des hyper-paramètres à fixer par validation croisée.\n",
    "\n",
    "Par ailleurs, comme je vais tester plusieurs modèles de régression et certains d'entre eux ne font que de l'univarié, je vais s'éparer la variable de réponse en deux variables. Les données numériques d'entrainement sont passées au log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f133afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_kwargs_models = {}\n",
    "dict_models = {}\n",
    "dict_param_grid = {}\n",
    "dict_log_param = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25708c7f",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Régression à noyaux \n",
    "\n",
    "Est-ce que le lien entre les données serait mieux représenté dans un RKHS ? \n",
    "\n",
    "Voilà la [page de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html) sur la régression à noyau. \n",
    "\n",
    "[Les différents noyaux implémentés dans sklearn](https://scikit-learn.org/stable/modules/metrics.html#metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "model_name = \"kernel_ridge\"\n",
    "\n",
    "dict_kwargs_models[model_name] = {\"alpha\":1, \"kernel\":'linear',\"gamma\":None,\n",
    "                                  \"degree\":3, \"coef0\":1, \"kernel_params\":None}\n",
    "dict_models[model_name] = KernelRidge(**dict_kwargs_models[model_name])\n",
    "dict_param_grid[model_name] = {\"alpha\":np.logspace(0, 1.2, 10), # regularisation param\n",
    "                               \"kernel\":[\"linear\", \"rbf\", #\"polynomial\",\n",
    "                                         \"laplacian\" ]}\n",
    "dict_log_param[\"alpha\"] = True\n",
    "dict_log_param[\"kernel\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702ef68",
   "metadata": {},
   "source": [
    "## 2. KNN :\n",
    "[page sklearn ](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "model_name = \"KNN\"\n",
    "\n",
    "dict_kwargs_models[model_name] = {\"n_neighbors\":5, \"weights\":'uniform', \"algorithm\":'auto',\n",
    "                                  \"leaf_size\":30, \"p\":2, \"metric\":'minkowski',\n",
    "                                  \"metric_params\":None, \"n_jobs\":None}\n",
    "dict_models[model_name] = KNeighborsRegressor(**dict_kwargs_models[model_name])\n",
    "dict_param_grid[model_name] = {\"n_neighbors\":np.arange(1,15,1)}\n",
    "dict_log_param[\"n_neighbors\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30010d5e",
   "metadata": {},
   "source": [
    "## 3. SVM : \n",
    "[page sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "model_name = \"SVR\"\n",
    "\n",
    "dict_kwargs_models[model_name] = {\"kernel\":'rbf', \"degree\":3, \"gamma\":'scale',\n",
    "                          \"coef0\":0.0, \"tol\":0.001, \"C\":1.0, \"epsilon\":0.1}\n",
    "dict_models[model_name] = SVR(**dict_kwargs_models[model_name])\n",
    "dict_param_grid[model_name] = {\"C\":np.logspace(-0.5, 3, 10),# regularisation param\n",
    "                               \"epsilon\":np.logspace(0,2,10)}\n",
    "dict_log_param[\"C\"] = True\n",
    "dict_log_param[\"epsilon\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8157896",
   "metadata": {},
   "source": [
    "Il faut que je modifie mon format de dict_param_grid pour qu'il devienne dictionnaire de dictionnaire (avec le nom des hyper-params et leur valeurs, pour chaque modèle\n",
    "\n",
    "Dans [l'aide de la fonction SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html), il y a une exemple d' utilisation de pipeline, à voir si ce n'est pas pertinent pour moi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd733e0c",
   "metadata": {},
   "source": [
    "## 4. Forêts aléatoires\n",
    "[page sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)\n",
    "\n",
    "et une [fiche sur les différentes méthodes ensemblistes ](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ed8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "model_name = \"random_forest\"\n",
    "\n",
    "dict_kwargs_models[model_name] = {\"n_estimators\":100, \n",
    "                                  \"criterion\":'mse', \"max_depth\":None, \n",
    "                                  \"min_samples_split\":2, \"min_samples_leaf\":1, \n",
    "                                  \"min_weight_fraction_leaf\":0.0, \"max_features\":'auto',\n",
    "                                  \"max_leaf_nodes\":None, \"min_impurity_decrease\":0.0,\n",
    "                                  \"min_impurity_split\":None, \"bootstrap\":True, \"oob_score\":False,\n",
    "                                  \"n_jobs\":None, \"random_state\":None, \"verbose\":0, \n",
    "                                  \"warm_start\":False, \"ccp_alpha\":0.0, \"max_samples\":None}\n",
    "dict_models[model_name] = RandomForestRegressor(**dict_kwargs_models[model_name])\n",
    "dict_param_grid[model_name] = {\"n_estimators\":np.arange(50,250,20)}# regularisation param\n",
    "dict_log_param[\"n_estimators\"]= False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5362a",
   "metadata": {},
   "source": [
    "## 5. Gradient Boosting\n",
    "[page sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model_name = \"gradient_boosting\"\n",
    "\n",
    "dict_kwargs_models[model_name]={\"loss\":'ls', \"learning_rate\":0.1, \"n_estimators\":100,\n",
    "                                \"subsample\":1.0, \"criterion\":'friedman_mse', \n",
    "                                \"min_samples_split\":2, \"min_samples_leaf\":1,\n",
    "                                \"min_weight_fraction_leaf\":0.0, \"max_depth\":3,\n",
    "                                \"min_impurity_decrease\":0.0, \"min_impurity_split\":None,\n",
    "                                \"init\":None, \"random_state\":None, \"max_features\":None, \n",
    "                                \"alpha\":0.9, \"verbose\":0, \"max_leaf_nodes\":None,\n",
    "                                \"warm_start\":False, \"validation_fraction\":0.1, \n",
    "                                \"n_iter_no_change\":None, \"tol\":0.0001, \"ccp_alpha\":0.0}\n",
    "\n",
    "dict_models[model_name] = GradientBoostingRegressor(**dict_kwargs_models[model_name])\n",
    "dict_param_grid[model_name]={\"n_estimators\":np.arange(50,250,20), \"learning_rate\":np.logspace(-2,0.1,10)}\n",
    "dict_log_param[\"learning_rate\"]= True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e04df",
   "metadata": {},
   "source": [
    "# Résultats et interprétation : \n",
    "## 1. Hyper-paramètres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b870930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CV_regression import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661dc46",
   "metadata": {},
   "source": [
    "Voilà le code pour lancer les différents modèles. J'ai enregistré les résultats dans deux fichiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_co2 = y[\"CO2_emissions\"]\n",
    "y_energy = y[\"SiteEnergyUse(kBtu)\"]\n",
    "\n",
    "my_y = y_energy\n",
    "y_name = \"energy\"\n",
    "# launch_CV(X_log_std, my_y, y_name ,dict_models,dict_param_grid, cv = 5)\n",
    "\n",
    "my_y = y_co2\n",
    "y_name = \"co2\"\n",
    "# launch_CV(X_log_std, my_y, y_name ,dict_models,dict_param_grid, cv = 5)\n",
    "\n",
    "## en échelle log sur y :\n",
    "y_co2 = np.log(y[\"CO2_emissions\"]+1)\n",
    "y_energy = np.log(y[\"SiteEnergyUse(kBtu)\"]+1)\n",
    "\n",
    "my_y = y_energy\n",
    "y_name = \"energy\"\n",
    "# launch_CV(X_log_std, my_y, y_name+\"_log\" ,dict_models,dict_param_grid, cv = 5)\n",
    "\n",
    "my_y = y_co2\n",
    "y_name = \"co2\"\n",
    "# launch_CV(X_log_std, my_y, y_name+\"_log\" ,dict_models,dict_param_grid, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7578589",
   "metadata": {},
   "source": [
    "## 2. Choix du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e745419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_y = y_energy\n",
    "# y_name = \"energy\"\n",
    "my_y = np.log(y_original_test[\"SiteEnergyUse(kBtu)\"]+1)\n",
    "y_name = \"energy_log\"\n",
    "\n",
    "import pickle\n",
    "with open(res_path+y_name+\"_dict_CV_res_reg.pkl\", 'rb') as fp:\n",
    "    dict_cv_results_energy = pickle.load(fp)\n",
    "with open(res_path+y_name+\"_dict_CV_best_params.pkl\", 'rb') as fp:\n",
    "    dict_best_params_energy = pickle.load(fp)  \n",
    "    \n",
    "for model_name in dict_models.keys(): \n",
    "    print(model_name)\n",
    "    res = dict_cv_results_energy[model_name]\n",
    "    plot_cv_res(res, dict_log_param)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bf8ca",
   "metadata": {},
   "source": [
    "Gardons donc les meilleurs hyper-paramètres : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_y_train = {\"co2\" : y[\"CO2_emissions\"] ,\n",
    "               \"co2_log\" : np.log(y[\"CO2_emissions\"]+1), \n",
    "               \"energy\" : y[\"SiteEnergyUse(kBtu)\"],\n",
    "               \"energy_log\" : np.log(y[\"SiteEnergyUse(kBtu)\"]+1)}\n",
    "\n",
    "dict_y_test = {\"co2\" : y_original_test[\"CO2_emissions\"] ,\n",
    "               \"co2_log\" : np.log(y_original_test[\"CO2_emissions\"]+1), \n",
    "               \"energy\" : y_original_test[\"SiteEnergyUse(kBtu)\"],\n",
    "               \"energy_log\" : np.log(y_original_test[\"SiteEnergyUse(kBtu)\"]+1)}\n",
    "\n",
    "y_name = \"co2\"\n",
    "my_y_train = dict_y_train[y_name]\n",
    "my_y_test = dict_y_test[y_name]\n",
    "my_X_train = X_log_std\n",
    "my_X_test = X_original_test_log_std\n",
    "\n",
    "dict_models = train_with_best_params(y_name, dict_models)\n",
    "\n",
    "predictions = get_prediction(dict_models, my_y_train, my_y_test, my_X_train, my_X_test)\n",
    "get_score(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = pd.Series(regressor.predict(X_original_test_log_std), name=model_name)\n",
    "predictions = get_prediction(dict_models,my_y,X_log_std,X_original_test_log_std)\n",
    "predictions = pd.concat((predictions,model_pred), axis = 1) \n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_y = y_co2\n",
    "y_name = \"co2\"\n",
    "\n",
    "import pickle\n",
    "with open(res_path+y_name+\"_dict_CV_res_reg.pkl\", 'rb') as fp:\n",
    "    dict_cv_results_co2 = pickle.load(fp)\n",
    "with open(res_path+y_name+\"_dict_CV_best_params.pkl\", 'rb') as fp:\n",
    "    dict_best_params_co2 = pickle.load(fp)  \n",
    "\n",
    "dict_models_co2 = dict_models.copy()    \n",
    "for model_name in dict_models.keys(): \n",
    "    best_param  = dict_best_params_co2[model_name]\n",
    "    regressor = dict_models_co2[model_name]\n",
    "    regressor.set_params(**best_param)\n",
    "    regressor.fit(X_log_std,my_y)\n",
    "    \n",
    "    dict_models_co2[model_name] = regressor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58d249",
   "metadata": {},
   "source": [
    "Les différents modèles, avec la meilleure paramétrisation est donc maintenant entrainée sur l'ensemble d'entrainement complet. Evaluons dont les pertes respectives de ces modèles sur l'ensemble test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f57497",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_original_test_log_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78088039",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d96ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab31d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e88ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05d6272e",
   "metadata": {},
   "source": [
    "# Pistes d'amélioration : \n",
    "* la regression linéaire non pénalisée me renvoit des scores trop négatifs -> j'ai fait un hack qui ne garde que les scores positifs, mais par la même occasion rendent les graphiques faux\n",
    "* faut-il standardiser les variables catégorielles ? -> s'il n'y a que des 0 et 1 on ne gagne rien ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ba305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "res_path = \"/home/clairegayral/Documents/openclassroom/res/P3/\"\n",
    "\n",
    "\n",
    "def get_dict_index(res):\n",
    "    '''\n",
    "    Get the index of different values in the results_cv dictionnary of GridSearchCV\n",
    "    '''\n",
    "    dict_index_values = {}\n",
    "    for param_name in res[\"params\"][0].keys() : \n",
    "        index_values = []\n",
    "        param_values = np.unique([dict_params[param_name] for dict_params in res[\"params\"] ])\n",
    "        for value in param_values :\n",
    "            index_value = [dict_params[param_name] == value for dict_params in res[\"params\"]]\n",
    "            index_values.append(index_value)\n",
    "        dict_index_values[param_name] = index_values\n",
    "    return(dict_index_values)\n",
    "\n",
    "def get_score_and_time(param_name, res, dict_index_values):\n",
    "    '''\n",
    "    Get mean score and the mean time for the parameter param_name, from res and dict_index_values\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    param_name : str of the parameter name\n",
    "    res : results_cv dictionnary of GridSearchCV\n",
    "    dict_index_values : result from get_dict_index(res)\n",
    "    \n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    mean score, mean time    \n",
    "    '''\n",
    "    mean_score_param = []\n",
    "    time_param = []\n",
    "    for index_value in dict_index_values[param_name] :\n",
    "        mean_score_param.append(np.mean(res[\"mean_test_score\"][index_value]))\n",
    "        time_tmp = (res[\"mean_fit_time\"] + res[\"mean_score_time\"])[index_value]\n",
    "        time_param.append(np.mean(time_tmp))\n",
    "    return(mean_score_param,time_param)\n",
    "\n",
    "def cv_plot_score_and_time(x, y_score, y_time, subplot = [1,2,1],\n",
    "                           param_name=\"\", x_log_scale = True):\n",
    "    '''\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    '''\n",
    "    subplot = subplot.copy()\n",
    "    plt.subplot(*subplot)\n",
    "    if x_log_scale : \n",
    "        plt.xscale(\"log\")\n",
    "    plt.plot(x, y_score)\n",
    "    plt.xlabel(\"hyperparam. \" + param_name)#get param name \n",
    "    plt.ylabel(\"score\")\n",
    "    plt.title(\"Score in Cross validation\")\n",
    "    subplot[2] += 1\n",
    "    plt.subplot(*subplot)\n",
    "    if x_log_scale : \n",
    "        plt.xscale(\"log\")\n",
    "    plt.plot(x, y_time)\n",
    "    plt.xlabel(\"hyperparam. \" + param_name)#get param name \n",
    "    plt.ylabel(\"time (s)\")\n",
    "    plt.title(\"Time in Cross validation\")\n",
    "\n",
    "def plot_cv_res(res,dict_log_param) :\n",
    "    '''\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    '''\n",
    "    dict_index_values = get_dict_index(res)\n",
    "    nb_params = len(res[\"params\"][0].keys())\n",
    "    plt.figure(figsize=(12,5*nb_params))\n",
    "    subplot_pos = [nb_params,2,1]\n",
    "\n",
    "    for param_name in res[\"params\"][0].keys() :\n",
    "        x = np.unique([dict_params[param_name] for dict_params in res[\"params\"]])\n",
    "        y_score, y_time = get_score_and_time(param_name, res, dict_index_values)\n",
    "        log_flag = dict_log_param[param_name]\n",
    "        cv_plot_score_and_time(x, y_score, y_time,subplot_pos, \n",
    "                               param_name= param_name, x_log_scale=log_flag)\n",
    "        subplot_pos[2]+= 2\n",
    "        \n",
    "def launch_CV(X, y, y_name ,dict_models,dict_param_grid, cv = 5):\n",
    "    '''\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    '''\n",
    "    ## some warnings mostly on dual gap\n",
    "    warnings.filterwarnings('ignore')\n",
    "    ## init \n",
    "    dict_cv_results_= {}\n",
    "    dict_best_params = {}\n",
    "    for model_name in dict_models.keys(): \n",
    "        print(model_name)\n",
    "        regressor = dict_models[model_name]\n",
    "        param_grid = dict_param_grid[model_name]\n",
    "        CV_regressor = GridSearchCV(regressor, param_grid, refit=True, cv = cv)\n",
    "        CV_regressor.fit(X, y)\n",
    "        dict_best_params[model_name] = CV_regressor.best_params_\n",
    "        res = CV_regressor.cv_results_\n",
    "        dict_cv_results_[model_name] = res\n",
    "        \n",
    "    with open(res_path+y_name+\"_dict_CV_res_reg.pkl\", 'wb') as fp:\n",
    "        pickle.dump(dict_cv_results_, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(res_path+y_name+\"_dict_CV_best_params.pkl\", 'wb') as fp:\n",
    "        pickle.dump(dict_best_params, fp, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "\n",
    "\n",
    "##        \n",
    "### apprentissage sur tout Xtrain : \n",
    "##\n",
    "\n",
    "def train_with_best_params(y_name, dict_models): \n",
    "    ## open best param dictionary\n",
    "    with open(res_path+y_name+\"_dict_CV_best_params.pkl\", 'rb') as fp:\n",
    "        dict_best_params = pickle.load(fp) \n",
    "    for model_name in dict_models.keys(): \n",
    "        best_param  = dict_best_params[model_name]\n",
    "        regressor = dict_models[model_name]\n",
    "        regressor.set_params(**best_param)\n",
    "        dict_models[model_name] = regressor\n",
    "    return(dict_models)  \n",
    "\n",
    "def get_prediction(dict_models, y_train_, y_test_, X_train_, X_test_):\n",
    "    ## init\n",
    "    predictions = y_test_.copy()\n",
    "    predictions = predictions.rename(\"real_\"+y_train_.name)\n",
    "    for model_name in dict_models.keys():\n",
    "        regressor = dict_models[model_name]\n",
    "        regressor.fit(X_train_,y_train_)\n",
    "        model_pred = pd.Series(regressor.predict(X_test_), \n",
    "                               name=model_name, index = X_test_.index)\n",
    "        predictions = pd.concat((predictions,model_pred), axis = 1) \n",
    "    return(predictions) \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "def get_score(predictions) : \n",
    "    scores = {}\n",
    "    for model_name in predictions.columns[1:] :\n",
    "        y_real =predictions.iloc[:,0]\n",
    "        y_pred = predictions[model_name]\n",
    "        scores[model_name] = r2_score(y_real, y_pred)\n",
    "    return(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclassroom",
   "language": "python",
   "name": "openclassroom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
