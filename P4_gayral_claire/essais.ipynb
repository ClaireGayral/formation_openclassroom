{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce341e93",
   "metadata": {},
   "source": [
    "# Modèles de classification\n",
    "\n",
    "Ce notebook contient les différents essais de modélisation que j'ai lancé pour ségmenter les clients. Ce sont des approches non supervisées :\n",
    "* modèle hiérarchique sur RMF\n",
    "* modèle DBSCAN \n",
    "* kmeans \n",
    "\n",
    "\n",
    "J'ai lancé ces modèles sur deux jeux de données extraits de l'analyse exploratoire. Le premier, constitué de 3 variables, est issu d'un modèle de marketing, le RMF. Le deuxième est l'aggrégation des différentes tables, à l'échelle des commandes, et je n'ai considéré que la dernière commande de chaque client (modèle qui exploite mieux les caractéristiques des différentes tables, mais est sans mémoire). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7769ce",
   "metadata": {},
   "source": [
    "# Reconstruction des tables\n",
    "\n",
    "Je propose de résumer le travail d'analyse exploiratoire présenté dans le premier notebook, pour en faire un script actionnable pour extraire les tables de travail. Ce travail est dans le script \"script00_P4_preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e50a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "data_path = \"/home/clairegayral/Documents/openclassroom/data/P4/\"\n",
    "res_path = \"/home/clairegayral/Documents/openclassroom/res/P4/\"\n",
    "\n",
    "## my .py : \n",
    "from script00_P4_preprocessing import *\n",
    "from script01_duplicates import *\n",
    "from script02_missing_values_treatment import *\n",
    "from script03_univariate_analysis import *\n",
    "from script04_multivariate_analysis import *\n",
    "from script05_CV_regression import *\n",
    "from script06_reduce_dim import *\n",
    "\n",
    "from sklearn import model_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ca0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "#### open data ####\n",
    "###################\n",
    "\n",
    "product_category_name_translation = pd.read_csv(data_path \n",
    "                        + \"product_category_name_translation.csv\")\n",
    "sellers = pd.read_csv(data_path + \"olist_sellers_dataset.csv\")\n",
    "products = pd.read_csv(data_path + \"olist_products_dataset.csv\")\n",
    "orders = pd.read_csv(data_path + \"olist_orders_dataset.csv\")\n",
    "order_reviews = pd.read_csv(data_path + \"olist_order_reviews_dataset.csv\")\n",
    "order_payments = pd.read_csv(data_path + \"olist_order_payments_dataset.csv\")\n",
    "order_items = pd.read_csv(data_path + \"olist_order_items_dataset.csv\")\n",
    "geolocation = pd.read_csv(data_path + \"olist_geolocation_dataset.csv\")\n",
    "customers = pd.read_csv(data_path + \"olist_customers_dataset.csv\")\n",
    "\n",
    "\n",
    "## Lien entre les tables :\n",
    "## order-product\n",
    "link_order_product = pd.merge(orders[\"order_id\"], \n",
    "    order_items[[\"order_id\",\"product_id\"]], \n",
    "    on = \"order_id\", how = 'right')\n",
    "link_order_product\n",
    "\n",
    "\n",
    "## customer-order\n",
    "link_customer_order = pd.merge(customers[[\"customer_unique_id\",\"customer_id\"]], \n",
    "    orders[[\"customer_id\",\"order_id\"]], \n",
    "    on = \"customer_id\", how = 'right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68ddda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#### Construction NMF ####\n",
    "##########################\n",
    "\n",
    "##\n",
    "## Recency\n",
    "##\n",
    "tmp = pd.merge(customers[[\"customer_id\",\"customer_unique_id\"]], \n",
    "               orders[[\"customer_id\", \"order_id\",\"order_purchase_timestamp\"]], \n",
    "               on=\"customer_id\", how=\"right\")\n",
    "## get the lastest order date of each customer \n",
    "customer_last_timestamp = tmp[[\"customer_unique_id\",\n",
    "           \"order_purchase_timestamp\"]].groupby(\"customer_unique_id\").max()\n",
    "## use datetime format\n",
    "customer_last_timestamp = pd.to_datetime(customer_last_timestamp[\"order_purchase_timestamp\"],\n",
    "                     format = \"%Y-%m-%d %H:%M:%S\")\n",
    "## substrack the date of the latest command in the data : \n",
    "t_max = customer_last_timestamp.max()\n",
    "recency = pd.Series(t_max-customer_last_timestamp, name = \"recency\")\n",
    "## get the difference in decimal days format : \n",
    "recency =  recency / np.timedelta64(1, \"D\")\n",
    "recency = recency.reset_index()\n",
    "\n",
    "rmf = recency\n",
    "\n",
    "##\n",
    "## Frequency\n",
    "##\n",
    "\n",
    "frequency = tmp.customer_unique_id.value_counts()\n",
    "frequency = pd.Series(frequency).reset_index()\n",
    "frequency = frequency.rename(columns={\"index\":\"customer_unique_id\",\n",
    "                                      \"customer_unique_id\":\"frequency\"})\n",
    "rmf = pd.merge(rmf, frequency, on=\"customer_unique_id\", how=\"left\")\n",
    "\n",
    "##\n",
    "## Monetary Value\n",
    "##\n",
    "\n",
    "tmp = pd.merge(tmp, order_payments[[\"order_id\",\"payment_value\"]], \n",
    "               on=\"order_id\", how=\"left\")\n",
    "monetary_value = tmp.groupby(\"customer_unique_id\").sum()\n",
    "monetary_value = monetary_value.reset_index()\n",
    "monetary_value = monetary_value.rename(columns={\"payment_value\":\"monetary_value\"})\n",
    "rmf = pd.merge(rmf, monetary_value, on=\"customer_unique_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c7fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#### construction table my_products ####\n",
    "########################################\n",
    "\n",
    "##\n",
    "## Product items : \n",
    "##\n",
    "def get_X_missing_vals_imputed(products, std = False):\n",
    "    ## extract numeric table with id as index\n",
    "    X = products.copy()\n",
    "    X = X.set_index(\"product_id\")\n",
    "    X = X.loc[:,~(X.dtypes == object)]\n",
    "    ## Standardize \n",
    "    my_std = preprocessing.StandardScaler().fit(X)\n",
    "    X_std = pd.DataFrame(my_std.transform(X), columns=X.columns, index=X.index)\n",
    "    n_neighbors = 10 \n",
    "    ## Impute missing values :\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    X_std = imputer.fit_transform(X_std)\n",
    "    X_std = pd.DataFrame(X_std, index = X.index, columns = X.columns)\n",
    "    if std :\n",
    "        return(X_std)\n",
    "    else :\n",
    "        ## Inverse-standardize : \n",
    "        X = my_std.inverse_transform(X_std)\n",
    "        X = pd.DataFrame(X, index = X_std.index, columns = X_std.columns)\n",
    "        return(X)\n",
    "def project2d_products(products, coeffs, var) :\n",
    "    dimension_vars, description_vars = var\n",
    "    dimension_coeffs = coeffs[dimension_vars]\n",
    "    description_coeffs = coeffs[description_vars]\n",
    "    X_std = get_X_missing_vals_imputed(products, std = True)\n",
    "    ## project on coeffs\n",
    "    product_dimension = np.dot(X_std.loc[:,dimension_vars],dimension_coeffs)\n",
    "    product_dimension = pd.Series(product_dimension, name=\"product_dimension\",\n",
    "                                 index = X_std.index)\n",
    "    product_description = np.dot(X_std.loc[:,description_vars],description_coeffs)\n",
    "    product_description = pd.Series(product_description, name=\"product_description\",\n",
    "                                 index = X_std.index)\n",
    "    ## concat on index = product_id\n",
    "    res = products.copy()\n",
    "    res = res.set_index(\"product_id\")\n",
    "#     res = pd.merge(res[\"product_category_name\"], product_dimens/ion, left_index=True, right_index=True)\n",
    "    res = pd.merge(product_dimension, product_description, left_index=True, right_index=True)\n",
    "    return(res)\n",
    "\n",
    "dimension_vars = ['product_weight_g', 'product_length_cm',       \n",
    "                  'product_height_cm', 'product_width_cm']\n",
    "description_vars = ['product_name_lenght', 'product_description_lenght',\n",
    "                    'product_photos_qty']\n",
    "coeffs = pd.read_csv(res_path+\"products_items_coeffs_PCA.csv\",index_col=0, squeeze=True)\n",
    "my_products = project2d_products(products, coeffs=coeffs,\n",
    "                                 var=(dimension_vars, description_vars))\n",
    "my_products = my_products.reset_index()\n",
    "# ##\n",
    "# ## Product category name\n",
    "# ##\n",
    "\n",
    "# y = products[[\"product_id\",\"product_category_name\"]]\n",
    "# y = y.set_index(\"product_id\").astype(\"object\")\n",
    "# y = y.fillna(\"missing\")\n",
    "# y = y.astype(\"category\")\n",
    "\n",
    "# all_cat = pd.Series(y[\"product_category_name\"].cat.categories, name =\"product_category_old\" )\n",
    "# new_cat = pd.Series(y[\"product_category_name\"].cat.categories, name =\"product_category_name\" )\n",
    "# for idx in all_cat.index :\n",
    "#     cat = all_cat[idx]\n",
    "#     new_cat.at[idx] = cat.split(\"_\")[0]\n",
    "# rename_cat = pd.merge(all_cat,new_cat, left_index=True, right_index=True)\n",
    "# rename_cat = rename_cat.set_index(\"product_category_old\").to_dict()\n",
    "# rename_cat['la']=\"cuisine\"\n",
    "# y = y.replace(rename_cat[\"product_category_name\"])\n",
    "# y = y.astype(\"category\").reset_index()\n",
    "# my_products = pd.merge(y,my_products, on=\"product_id\",how=\"left\")\n",
    "\n",
    "##\n",
    "## Ordered product\n",
    "##\n",
    "\n",
    "## descriptive statistic : how often it is ordered ? \n",
    "product_nb_app = order_items.product_id.value_counts()\n",
    "my_quantile_classes = {\"very_low\":[1,0.95], \"low\":[0.95,0.75],\n",
    "                       \"medium_low\" : [0.75,0.5],\"medium_high\" : [0.5,0.25],\n",
    "                       \"high\" : [0.25,0.05], \"very_high\" : [0.05,0]}\n",
    "my_products.loc[:,\"product_freq_buy\"] = 0\n",
    "for freq_class in my_quantile_classes.keys():\n",
    "    q_max, q_min = my_quantile_classes[freq_class] \n",
    "    n_max = product_nb_app.quantile(q=q_max)\n",
    "    n_min = product_nb_app.quantile(q=q_min)\n",
    "    if ~(n_min == n_max) : \n",
    "        cond_max = np.where(product_nb_app<=n_max)\n",
    "        cond_min = np.where(product_nb_app>n_min)\n",
    "        if q_max == 1 :\n",
    "            range_index = cond_max\n",
    "        elif q_min == 0 :\n",
    "            range_index = cond_min\n",
    "        else :\n",
    "            range_index = np.intersect1d(cond_min,cond_max)\n",
    "        product_in_class = order_items.product_id.iloc[range_index].values\n",
    "        product_class_index = my_products.loc[my_products[\"product_id\"].isin(product_in_class),:].index\n",
    "        my_products.at[product_class_index,\"product_freq_buy\"] = np.mean([q_min,q_max])        \n",
    "## ordered alone\n",
    "my_products.loc[:,\"product_flag_ordered_alone\"] = 0\n",
    "range_index = np.where(order_items.product_id.value_counts()==1)\n",
    "products_ordered_alone = order_items.product_id.iloc[range_index].values\n",
    "products_ordered_alone = my_products.index.isin(products_ordered_alone)\n",
    "my_products.at[products_ordered_alone,\"product_flag_ordered_alone\"] = 1\n",
    "## prices\n",
    "tmp = order_items.groupby(\"product_id\").mean()[['price', 'freight_value']]\n",
    "tmp = tmp.add_prefix(\"product_\")\n",
    "tmp = tmp.reset_index()\n",
    "my_products = pd.merge(my_products,tmp, on=\"product_id\",how=\"left\")\n",
    "\n",
    "##\n",
    "## Product category\n",
    "##\n",
    "\n",
    "rename_categories_english = {\n",
    "    \"home_furnitures\" : ['bed_bath_table','furniture_decor', \n",
    "                         'housewares','office_furniture',\n",
    "                         'kitchen_dining_laundry_garden_furniture',\n",
    "                         'home_confort','furniture_mattress_and_upholstery',\n",
    "                         'furniture_living_room', 'furniture_bedroom',\n",
    "                         'home_comfort_2',\n",
    "                        ],\n",
    "    \"home_electronics\":['small_appliances','air_conditioning',\n",
    "                        'home_appliances','home_appliances_2',\n",
    "                        'la_cuisine','small_appliances_home_oven_and_coffee',\n",
    "                        \"kitchen_portables_and_food_preparers\"\n",
    "                       ],\n",
    "    \"electronics\":['computers_accessories','telephony',\n",
    "                   'tablets_printing_image', 'fixed_telephony',\n",
    "                   'consoles_games', 'audio','electronics',\n",
    "                   'computers',\n",
    "                  ],\n",
    "    \"multimedia\" : ['books_general_interest','books_imported',\n",
    "                    'cine_photo','music', \n",
    "                    'cds_dvds_musicals', 'dvds_blu_ray',\n",
    "                   ], \n",
    "    \"fashion\" : ['fashion_bags_accessories','fashion_shoes',\n",
    "                 'fashion_male_clothing','fashion_underwear_beach',\n",
    "                 'fashion_sport', 'fashio_female_clothing',\n",
    "                ],\n",
    "    \"children\" : [ 'baby','toys',\n",
    "                   'fashion_childrens_clothes'\n",
    "                 ],\n",
    "    \"health\" : ['health_beauty', 'perfumery',\n",
    "                'diapers_and_hygiene'\n",
    "               ],\n",
    "    \"food_drink\" : ['food_drink','market_place',\n",
    "                    'agro_industry_and_commerce','food',\n",
    "                    'drinks'\n",
    "                    ],\n",
    "    \"leisure\" : ['auto','sports_leisure',\n",
    "                 'watches_gifts',  'stationery',\n",
    "                 'luggage_accessories', 'pet_shop',\n",
    "                 'party_supplies','musical_instruments',\n",
    "                 'arts_and_craftmanship',\n",
    "                ],\n",
    "    \"decoration\" : ['cool_stuff','art',\n",
    "                    'christmas_supplies','flowers',\n",
    "                   ],\n",
    "    \"DIY\" : ['garden_tools','construction_tools_construction',\n",
    "             'costruction_tools_garden','costruction_tools_tools', \n",
    "             'books_technical','home_construction',\n",
    "             'construction_tools_lights','construction_tools_safety',\n",
    "             'industry_commerce_and_business',\n",
    "            ],\n",
    "    \"security\" : ['signaling_and_security','security_and_services']\n",
    "    }\n",
    "\n",
    "rename_cat = product_category_name_translation.copy()\n",
    "## manquait 2 variables dans la table de traduction :\n",
    "rename_cat = rename_cat.append({\"product_category_name\":\"portateis_cozinha_e_preparadores_de_alimentos\",\n",
    "                   \"product_category_name_english\" : \"kitchen_portables_and_food_preparers\"},\n",
    "                   ignore_index=True)\n",
    "rename_cat = rename_cat.append({\"product_category_name\": \"pc_gamer\",\n",
    "                   \"product_category_name_english\" : \"pc_gamer\"},\n",
    "                   ignore_index=True)\n",
    "for new_cat, list_old_cat in rename_categories_english.items():\n",
    "    bool_idx = rename_cat[\"product_category_name_english\"].isin(list_old_cat)\n",
    "    cat_idx = rename_cat.loc[bool_idx].index\n",
    "    rename_cat.at[cat_idx, \"new_cat_english\"] = new_cat\n",
    "dict_rename_cat = rename_cat[[\"product_category_name\",\"new_cat_english\"]]\n",
    "dict_rename_cat = dict_rename_cat.set_index(\"product_category_name\")\n",
    "dict_rename_cat = dict_rename_cat.to_dict()[\"new_cat_english\"]\n",
    "\n",
    "y = products[[\"product_id\",\"product_category_name\"]]\n",
    "# y = y.set_index(\"product_id\")\n",
    "y = y.replace(dict_rename_cat)\n",
    "y = y.astype(\"category\")\n",
    "\n",
    "if \"product_category_name\" in my_products.columns:\n",
    "    my_products = my_products.drop(columns=\"product_category_name\")\n",
    "my_products = pd.merge(y,my_products, on=\"product_id\",how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ac6ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id                      object\n",
       "product_category_name         category\n",
       "product_dimension              float64\n",
       "product_description            float64\n",
       "product_freq_buy               float64\n",
       "product_flag_ordered_alone       int64\n",
       "product_price                  float64\n",
       "product_freight_value          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_products.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "870461a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#### construction table my_orders ####\n",
    "######################################\n",
    "\n",
    "##\n",
    "## order_status\n",
    "##\n",
    "\n",
    "my_orders = orders[['order_id', 'customer_id']].copy()\n",
    "y = orders[['order_id','order_status']]\n",
    "y = y.set_index(\"order_id\").astype(\"category\")\n",
    "my_orders = pd.merge(my_orders, y, on=\"order_id\", how=\"left\")\n",
    "\n",
    "##\n",
    "## Time and dates\n",
    "##\n",
    "\n",
    "order_dates = pd.DataFrame(index=orders.order_id)\n",
    "## Purchase date and time split : \n",
    "purchase_timestamp = pd.to_datetime(\n",
    "        orders.order_purchase_timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "purchase_date = pd.to_datetime(\n",
    "        purchase_timestamp.dt.date, format=\"%Y-%m-%d\")\n",
    "order_dates.at[:,\"order_purchase_date\"] = purchase_date.values\n",
    "purch_time = purchase_timestamp.dt.time \n",
    "order_dates.at[:,\"order_purchase_time\"] = purch_time.values\n",
    "order_dates = order_dates.astype({\"order_purchase_date\":\"object\",\n",
    "                                  \"order_purchase_time\":\"object\"})\n",
    "## Delta time for delivery date comparison (ctm = customer)\n",
    "estim_delivery_date = pd.to_datetime(\n",
    "        orders.order_estimated_delivery_date, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "delivered_ctm_date = pd.to_datetime(\n",
    "        orders.order_delivered_customer_date, format=\"%Y-%m-%d %H:%M:%S\") \n",
    "delivered_carrier_date = pd.to_datetime(\n",
    "        orders.order_delivered_carrier_date, format=\"%Y-%m-%d %H:%M:%S\") \n",
    "delta_estim_declared = estim_delivery_date - delivered_ctm_date\n",
    "delta_estim_declared = delta_estim_declared / np.timedelta64(1, \"D\")\n",
    "order_dates.at[:,\"order_dt_estim_declared\"] = delta_estim_declared.values\n",
    "delta_ctm_carrier = delivered_ctm_date - delivered_carrier_date\n",
    "delta_ctm_carrier = delta_ctm_carrier / np.timedelta64(1, \"D\")\n",
    "order_dates.at[:,\"order_dt_ctm_carrier\"] = delta_ctm_carrier.values\n",
    "my_orders = pd.merge(my_orders, order_dates, on=\"order_id\",how=\"left\")\n",
    "\n",
    "##\n",
    "## Prices \n",
    "##\n",
    "\n",
    "tmp = order_items.groupby(\"order_id\").sum()\n",
    "tmp = tmp.reset_index().drop(columns=\"order_item_id\")\n",
    "my_orders = pd.merge(my_orders, tmp, on=\"order_id\",how=\"left\")\n",
    "\n",
    "##\n",
    "## Payments\n",
    "##\n",
    "\n",
    "## payment_installments :\n",
    "tmp = order_payments.sort_values(\"payment_installments\",ascending=False)\n",
    "tmp = tmp.drop_duplicates(\"order_id\",\"first\")\n",
    "my_orders = pd.merge(my_orders, tmp[[\"order_id\",\"payment_installments\"]], \n",
    "                     on=\"order_id\", how=\"left\")\n",
    "## most of payment type (max sum of value): \n",
    "# extract sum of payment value for each payment type : \n",
    "sub_table = order_payments[[\"order_id\",\"payment_value\",\"payment_type\"]]\n",
    "sub_table = sub_table.sort_values(\"order_id\")\n",
    "index = sub_table.drop_duplicates([\"order_id\",\"payment_type\"]).index\n",
    "tmp = sub_table.groupby([\"order_id\",\"payment_type\"]).sum()\n",
    "tmp = tmp.reset_index().set_index(index)\n",
    "## drop duplicates where smaller payment_values (sum)\n",
    "tmp = tmp.sort_values(\"payment_value\", ascending=False)\n",
    "tmp = tmp.drop_duplicates(\"order_id\", keep=\"first\")\n",
    "payment_type = tmp.copy()\n",
    "y = tmp[['order_id','payment_type']]\n",
    "y = y.astype({\"payment_type\":\"category\"}) \n",
    "my_orders = pd.merge(my_orders, y, on=\"order_id\", how=\"left\")\n",
    "## number of payment type : \n",
    "tmp = order_payments[[\"order_id\",\"payment_type\"]].drop_duplicates()\n",
    "tmp = tmp[\"order_id\"].value_counts().reset_index()\n",
    "tmp = tmp.rename(columns={\"index\":\"order_id\", \"order_id\":\"nb_payment_type\"})\n",
    "my_orders = pd.merge(my_orders, tmp, on=\"order_id\", how=\"left\")\n",
    "\n",
    "##\n",
    "## Reviews \n",
    "##\n",
    "\n",
    "## add count_reviews, count_messages, count_title, for one order : \n",
    "tmp = order_reviews.groupby(\"order_id\").count().reset_index()\n",
    "tmp = tmp[[\"order_id\",\"review_id\", \"review_comment_title\", \"review_comment_message\"]]\n",
    "# renaming columns (\"count\" instead of \"review\")\n",
    "dict_rename = {\"review_id\":\"count_review\"}\n",
    "for colname in tmp.columns[2:] :\n",
    "    dict_rename[colname] = colname.replace(\"review\",\"count\")\n",
    "tmp = tmp.rename(columns=dict_rename)\n",
    "my_orders = pd.merge(my_orders, tmp, on=\"order_id\", how=\"left\")\n",
    "\n",
    "##\n",
    "## Feature issues des variables numériques de produits\n",
    "##\n",
    "tmp = pd.merge(link_order_product, my_products, \n",
    "               on=\"product_id\", how=\"left\")\n",
    "tmp = tmp.drop(columns=['product_id',\"product_flag_ordered_alone\",\n",
    "                        \"product_price\", \"product_freight_value\"])\n",
    "tmp = tmp.groupby(\"order_id\").sum()\n",
    "tmp = tmp.add_prefix(\"sum_\")\n",
    "tmp = tmp.reset_index()\n",
    "my_orders = pd.merge(my_orders,tmp, on=\"order_id\", how=\"left\")\n",
    "\n",
    "##\n",
    "## Feature issues des categories de produits\n",
    "##\n",
    "\n",
    "## extract dummies \n",
    "product_cat_dummies = pd.get_dummies(my_products[\"product_category_name\"])\n",
    "product_cat_dummies = product_cat_dummies.add_prefix(\"product_category_\")\n",
    "product_cat_dummies = product_cat_dummies.set_index(products[\"product_id\"]).reset_index()\n",
    "## get nb of categories in the same product\n",
    "order_cat = pd.merge(link_order_product,product_cat_dummies, on=\"product_id\", how=\"left\")\n",
    "order_diff_cat = order_cat.drop(columns=\"product_id\")\n",
    "order_diff_cat = order_diff_cat.groupby(\"order_id\").sum()\n",
    "order_diff_cat[order_diff_cat>1] = 1\n",
    "tmp = order_diff_cat.sum(axis=1)\n",
    "tmp = tmp.reset_index().rename(columns={0:\"count_prod_cat\"})\n",
    "my_orders = pd.merge(my_orders, tmp, on=\"order_id\", how=\"left\")\n",
    "## get nb max of product in the same category\n",
    "order_cat_no_duplc = order_cat.drop_duplicates(subset=[\"order_id\",\"product_id\"])\n",
    "order_cat_no_duplc = order_cat_no_duplc.drop(columns=\"product_id\")\n",
    "order_cat_no_duplc = order_cat.groupby(\"order_id\").sum()\n",
    "order_cat_no_duplc.sum(axis=0)\n",
    "tmp = order_cat_no_duplc.max(axis=1)\n",
    "tmp = tmp.rename(\"count_max_product_in_cat\")\n",
    "\n",
    "\n",
    "## to compute one hot encoder, finally keep categorical for the moment\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# enc.fit(y)\n",
    "# y_dummies = pd.DataFrame(enc.transform(y).toarray(),\n",
    "#                           index = y.index, columns=np.unique(y))\n",
    "# y_dummies = y_dummies.add_prefix(\"order_status_\").reset_index()\n",
    "# y_dummies = y_dummies.astype(\"category\")\n",
    "# my_orders = pd.merge(my_orders, y_dummies, on=\"order_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "408d4c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id                     object\n",
       "customer_id                  object\n",
       "order_status               category\n",
       "order_purchase_date          object\n",
       "order_purchase_time          object\n",
       "order_dt_estim_declared     float64\n",
       "order_dt_ctm_carrier        float64\n",
       "price                       float64\n",
       "freight_value               float64\n",
       "payment_installments        float64\n",
       "payment_type               category\n",
       "nb_payment_type             float64\n",
       "count_review                  int64\n",
       "count_comment_title           int64\n",
       "count_comment_message         int64\n",
       "sum_product_dimension       float64\n",
       "sum_product_description     float64\n",
       "sum_product_freq_buy        float64\n",
       "count_prod_cat              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#### Construction table my_customers ####\n",
    "#########################################\n",
    "\n",
    "tmp = customers.drop_duplicates(subset=\"customer_unique_id\")\n",
    "my_customers = tmp[[\"customer_unique_id\",\"customer_zip_code_prefix\"]]\n",
    "\n",
    "my_customers = pd.merge(my_customers, rmf, on=\"customer_unique_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_customers.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclassroom",
   "language": "python",
   "name": "openclassroom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
