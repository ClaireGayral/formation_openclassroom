{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "data_path = \"/home/clairegayral/Documents/openclassroom/data/P5/\"\n",
    "res_path = \"/home/clairegayral/Documents/openclassroom/res/P5/\"\n",
    "\n",
    "## my .py : \n",
    "from script01_duplicates import *\n",
    "from script02_missing_values_treatment import *\n",
    "from script03_univariate_analysis import *\n",
    "from script04_multivariate_analysis import *\n",
    "from script05_CV_regression import *\n",
    "from script06_reduce_dim import *\n",
    "\n",
    "## extract list of text :\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# from sklearn.feature_extraction.s import s# import ENGLISH_STOP_WORDS\n",
    "\n",
    "# from sklearn import model_selection \n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import neighbors\n",
    "# from sklearn import linear_model\n",
    "\n",
    "# from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import fbeta_score\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import fowlkes_mallows_score\n",
    "# from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from scipy.cluster.hierarchy import cut_tree\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.utils import tokenize\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-principle",
   "metadata": {},
   "source": [
    "# Mission\n",
    "\n",
    "Dans un premier notebook, les posts tagués ont été extraits, et prétraités. De ce travail, les textes sont laissés sous forme de \"bag of word\", une liste de mots d'intérêt appelés tokens. Pour les variables de sortie dans l'apprentissage supervisé, seulement quelques tags sont gardés, afin de limiter la complexité de calculs. Il sera possible de généraliser cela.\n",
    "\n",
    "## Rappel des consignes :\n",
    "\n",
    "* Mettre en œuvre une approche non supervisée.\n",
    "* Utiliser une approche supervisée ou non pour extraire des tags à partir des résultats précédents.\n",
    "* Comparer ses résultats à une approche purement supervisée, après avoir appliqué des méthodes d’extraction de features spécifiques des données textuelles.\n",
    "* Mettre en place une méthode d’évaluation propre, avec une séparation du jeu de données pour l’évaluation.\n",
    "* Pour suivre les modifications du code final à déployer, utiliser un logiciel de gestion de versions, par exemple Git.\n",
    "\n",
    "\n",
    "## Livrable attendu ici\n",
    "Un notebook de test de différents modèles (non cleané, pour comprendre votre démarche)\n",
    "\n",
    "\n",
    "# Ressources : \n",
    "## LDA \n",
    "Une explication générale : https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2\n",
    "\n",
    "\n",
    "## Gradient Boosting :\n",
    "\n",
    "# Plan du notebook :\n",
    "\n",
    "Pour faciliter la lecture, il est possible de revenir à ce sommaire, en cliquant sur [back to menu](#menu) chaque fois qu'il apparaît. A partir de ce menu, il est possible de naviguer entre les parties en cliquant sur leur titre.\n",
    "\n",
    "\n",
    "<a id =\"menu\"></a>\n",
    "1. [Les posts](#section1)\n",
    "    1. [Import du travail précédent](#section1.1)\n",
    "    2. [Séparation des données](#section1.2)\n",
    "    \n",
    "2. [Approche non supervisée](#section2)\n",
    "    0. [Random Forest](#section2.0)\n",
    "    1. [LDA - apprentissage](#section2.1)\n",
    "    2. [LDA - Prédiction](#section2.2)\n",
    "    \n",
    "3. [Approche supervisée](#section3)\n",
    "    1. [Gradient Boosting](#section1.2)\n",
    "        2. [](#section3.1.1)\n",
    "        3. [](#section3.1.2)\n",
    "        4. [](#section3.1.3)   \n",
    "    2. [Bert ?](#section3.2)\n",
    "        1. [](#section3.2.1)\n",
    "        2. [](#section3.2.2)\n",
    "        3. [](#section3.2.3)       \n",
    "        4. [](#section3.2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-flour",
   "metadata": {},
   "source": [
    "<a id =\"section1\"></a>\n",
    "<a id =\"section1.1\"></a>\n",
    "\n",
    "[back to menu](#menu)\n",
    "\n",
    "# 1.Les posts\n",
    "## 1.A Import du travail précédent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alpha-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Posts Lemmatized :\n",
    "with open(res_path+\"posts_cleanned_corpora_tokens\"+\".pkl\", \"rb\") as f :\n",
    "    corpora = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "demonstrated-compact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convert',\n",
       " 'decimal',\n",
       " 'double',\n",
       " 'track',\n",
       " 'bar',\n",
       " 'change',\n",
       " 'form',\n",
       " 'opacity',\n",
       " 'decimal',\n",
       " 'trackbar',\n",
       " 'opacity',\n",
       " 'tran',\n",
       " 'build',\n",
       " 'give',\n",
       " 'follow',\n",
       " 'implicitly',\n",
       " 'convert',\n",
       " 'decimal',\n",
       " 'double',\n",
       " 'try',\n",
       " 'tran',\n",
       " 'double',\n",
       " 'work',\n",
       " 'fine',\n",
       " 'vb']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text.split(\" \") for text in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "defined-registrar",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_149769/1794769348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env_python/openclassroom/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env_python/openclassroom/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         ]\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env_python/openclassroom/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.env_python/openclassroom/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame({k:text.split(\" \") for k,text in corpora.items()})\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tags :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-intervention",
   "metadata": {},
   "source": [
    "<a id =\"section1.2\"></a>\n",
    "\n",
    "[back to menu](#menu)\n",
    "\n",
    "## 1.B Séparation des données\n",
    "\n",
    "Pour comparer les données, je vais séparer mon ensemble de posts en apprentissage et test. Le premier servira à apprendre les modèles, le deuxième à comparer les prédictions entre les différents modèles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclassroom",
   "language": "python",
   "name": "openclassroom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
